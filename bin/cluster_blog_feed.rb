#!/usr/bin/env ruby

require 'rubygems'
require 'log4r'

require 'rss'
require 'sclust/doccluster'
require 'net/http'
require 'uri'
require 'optparse'
require 'mechanize'

Log4r::Logger::root.level = 0
$logger = Log4r::Logger.new($0)
$logger.outputters = [ Log4r::StderrOutputter.new($0) ]

$wwwagent = WWW::Mechanize.new()

config = { :opmlFiles=>[], :urlHashes => [], :topTerms => 10}

OptionParser.new() do |opt|
    opt.banner = "Use of #{$0}:"
    
    opt.on("-o", "--opml=String", "OPML file to read blog feeds in from.") do |v|
        config[:opmlFiles] << v
    end
    
    opt.on("-t", "--terms=Integer", Integer, "Number of top-terms per cluster to display.") do |v|
        config[:topTerms] = v
    end
    
    opt.on("-h", "--help", "This menu") do |v|
        puts(opt)
        exit 1
    end
end.parse!

puts(config[:topTerms])

# Parse an OPML file generated by Google Reader and return a list of hashes where
# the keys are the strings 'url', 'xmlUrl', and 'htmlUrl'.
def parse_opml_file(file)
    r = []
    
    f = File.new(file)
    
    doc = REXML::Document.new(f)
    
    doc.root.each_element('//outline[@type]') { |ele| r << ele.attributes if ele.attributes.has_key?('xmlUrl') }

    f.close
    
    r
end

# Takes an RSS document (or atom document),
# a URI object to one, a URI string, or an XML string
# that will be parsed into a document.
#
# The resulting object will be iterated through and the items
# put into the document collection.
def rss_to_documents(documentCollection, rss)
    
    $logger.debug("Operating on #{rss} of type #{rss.class}")
    
    # This block builds an RSS::Element (document).
    unless (rss.instance_of?(RSS::Element))
        
        # Check if we have a URI string...
        if ( rss.instance_of?(String) )
            begin
                rss = URI.parse(rss)
            rescue URI::InvalidURIError => e
                $logger.warning("Exception parsing URI: #{e.message}")
            end
        end
        
        $logger.debug("Rss is now of type #{rss.class}.")

        # Parse it...
        if (rss.instance_of?(URI::HTTP))
            begin
                #rss = RSS::Parser::parse(Net::HTTP::get(rss), false)
                rss = RSS::Parser::parse($wwwagent.get_file(rss), false)
            rescue Exception => e
                $logger.error("Failed to retrieve URL #{rss}: #{e.message}")
                throw e
            end
        elsif(rss.instance_of?(String))
            rss = RSS::Parser::parse(rss, false)
        else
            rss = nil
        end
        
        throw Exception.new("RSS was not a URI string, a URI object, an RSS document, or an RSS document string: #{rss}") unless rss
    end
    
    unless ( rss.nil? || rss.items.nil? )
    
        $logger.debug("Adding #{rss.items.size} to document collection.")
    
        # Add this documents of this item to the document collection.
        rss.items.each do |item|
            
            # Simply little temporary helper call to handle creation / erorr checking of cluster documents.
            addNewDoc = lambda do |title, body|
                if ( body )
                    $logger.debug("Adding item #{title}")
                    documentCollection + SClust::Document.new(body, :userData=>item, :ngrams=>[1,2,3])
                else
                    $logger.warn("No body for post #{title}")
                end
            end
            
            
            if ( item.instance_of?(RSS::Rss::Channel::Item))
                
                addNewDoc.call(item.title, item.description) if ( item.description )
                
            elsif ( item.instance_of?(RSS::RDF::Item) )
                
                addNewDoc.call(item.title, item.content_encoded)

            else
                
                addNewDoc.call(item.title.content, item.content.content)

            end
        end
    end
end

config[:opmlFiles].each { |file| config[:urlHashes] += parse_opml_file(file) }

col = SClust::DocumentCollection.new()

col.logger.outputters = $logger.outputters

count = 1

config[:urlHashes].each do |url|

    $logger.info("Processing #{url['title']} #{count} / #{config[:urlHashes].size}")

    begin
        rss_to_documents(col, url['xmlUrl'])
    rescue Exception => e
        $logger.error("Error retrieving #{url['xmlUrl']}: #{e.message}. Skipping.")
        $logger.error(e.backtrace.join("\n"))
    end

    count += 1

end

#col.drop_terms(0.0001, 0.10) # Removed 14 words
#col.drop_terms(0.0001, 0.05)

cluster = SClust::DocumentClusterer.new(col)

cluster.iterations=3

cluster.logger.outputters = $logger.outputters

cluster.cluster()

cluster.each_cluster do |cluster| 
    puts("---------- Cluster #{cluster} ---------- ")
    cluster.get_max_terms(config[:topTerms]).each do |term|
        print("\tTerm:(#{term}=#{cluster.center.values[term]})")
    end
    puts("")
end


